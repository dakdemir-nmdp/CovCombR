<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="CovCombR Package" />

<meta name="date" content="2025-10-26" />

<title>Identifiability, Error Analysis, and Power Analysis for Covariance Matrix Estimation from Partial Observations</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Identifiability, Error Analysis, and Power
Analysis for Covariance Matrix Estimation from Partial Observations</h1>
<h4 class="author">CovCombR Package</h4>
<h4 class="date">2025-10-26</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#setup-and-notation" id="toc-setup-and-notation"><span class="toc-section-number">2</span> Setup and Notation</a>
<ul>
<li><a href="#formal-problem-statement" id="toc-formal-problem-statement"><span class="toc-section-number">2.1</span> Formal Problem Statement</a></li>
<li><a href="#likelihood-function" id="toc-likelihood-function"><span class="toc-section-number">2.2</span> Likelihood Function</a></li>
</ul></li>
<li><a href="#graph-structure-and-information-flow" id="toc-graph-structure-and-information-flow"><span class="toc-section-number">3</span> Graph Structure and Information
Flow</a>
<ul>
<li><a href="#definition-path-connectivity" id="toc-definition-path-connectivity"><span class="toc-section-number">3.1</span> Definition: Path
Connectivity</a></li>
<li><a href="#imputation-structure-em-algorithm" id="toc-imputation-structure-em-algorithm"><span class="toc-section-number">3.2</span> Imputation Structure (EM
Algorithm)</a></li>
</ul></li>
<li><a href="#main-identifiability-theorem" id="toc-main-identifiability-theorem"><span class="toc-section-number">4</span> Main Identifiability Theorem</a>
<ul>
<li><a href="#observation-hypergraph-and-clique-coverage" id="toc-observation-hypergraph-and-clique-coverage"><span class="toc-section-number">4.1</span> Observation Hypergraph and Clique
Coverage</a></li>
<li><a href="#theorem-local-identifiability-via-chordal-cliques" id="toc-theorem-local-identifiability-via-chordal-cliques"><span class="toc-section-number">4.2</span> Theorem: Local Identifiability via
Chordal Cliques</a></li>
<li><a href="#proof-of-theorem-3.1" id="toc-proof-of-theorem-3.1"><span class="toc-section-number">4.3</span> Proof of Theorem 3.1</a></li>
</ul></li>
<li><a href="#error-analysis-and-rate-of-convergence" id="toc-error-analysis-and-rate-of-convergence"><span class="toc-section-number">5</span> Error Analysis and Rate of
Convergence</a>
<ul>
<li><a href="#information-decomposition-and-asymptotic-variance" id="toc-information-decomposition-and-asymptotic-variance"><span class="toc-section-number">5.1</span> Information Decomposition and
Asymptotic Variance</a></li>
<li><a href="#em-convergence-and-missing-information" id="toc-em-convergence-and-missing-information"><span class="toc-section-number">5.2</span> EM Convergence and Missing
Information</a></li>
<li><a href="#positive-definiteness-constraint-and-information-region" id="toc-positive-definiteness-constraint-and-information-region"><span class="toc-section-number">5.3</span> Positive Definiteness Constraint
and Information Region</a></li>
</ul></li>
<li><a href="#bias-and-mean-squared-error" id="toc-bias-and-mean-squared-error"><span class="toc-section-number">6</span> Bias and Mean Squared Error</a>
<ul>
<li><a href="#bias-in-em-estimates" id="toc-bias-in-em-estimates"><span class="toc-section-number">6.1</span> Bias in EM Estimates</a></li>
<li><a href="#mean-squared-error-decomposition" id="toc-mean-squared-error-decomposition"><span class="toc-section-number">6.2</span> Mean Squared Error
Decomposition</a></li>
</ul></li>
<li><a href="#plugin-vs.-bootstrap-standard-errors" id="toc-plugin-vs.-bootstrap-standard-errors"><span class="toc-section-number">7</span> Plugin vs. Bootstrap Standard
Errors</a>
<ul>
<li><a href="#plugin-se-limitation" id="toc-plugin-se-limitation"><span class="toc-section-number">7.1</span> Plugin SE Limitation</a></li>
<li><a href="#bootstrap-se-validity" id="toc-bootstrap-se-validity"><span class="toc-section-number">7.2</span> Bootstrap SE Validity</a></li>
</ul></li>
<li><a href="#application-to-3x3-example" id="toc-application-to-3x3-example"><span class="toc-section-number">8</span> Application to 3x3 Example</a>
<ul>
<li><a href="#explicit-formulas-for-3x3-case" id="toc-explicit-formulas-for-3x3-case"><span class="toc-section-number">8.1</span> Explicit Formulas for 3x3
Case</a></li>
<li><a href="#effective-sample-size-interpretation" id="toc-effective-sample-size-interpretation"><span class="toc-section-number">8.2</span> Effective Sample Size
Interpretation</a></li>
<li><a href="#illustrative-r-code-for-bootstrap-se" id="toc-illustrative-r-code-for-bootstrap-se"><span class="toc-section-number">8.3</span> Illustrative R Code for Bootstrap
SE</a></li>
</ul></li>
<li><a href="#summary-theorem" id="toc-summary-theorem"><span class="toc-section-number">9</span> Summary Theorem</a></li>
<li><a href="#conditions-for-validity-and-breakdown-cases" id="toc-conditions-for-validity-and-breakdown-cases"><span class="toc-section-number">10</span> Conditions for Validity and
Breakdown Cases</a>
<ul>
<li><a href="#when-theorems-apply-strictly" id="toc-when-theorems-apply-strictly"><span class="toc-section-number">10.1</span> When Theorems Apply
Strictly</a></li>
<li><a href="#boundary-cases-where-theory-breaks-down" id="toc-boundary-cases-where-theory-breaks-down"><span class="toc-section-number">10.2</span> Boundary Cases Where Theory
Breaks Down</a></li>
</ul></li>
<li><a href="#mathematical-appendix-lemmas-and-proofs" id="toc-mathematical-appendix-lemmas-and-proofs"><span class="toc-section-number">11</span> Mathematical Appendix: Lemmas and
Proofs</a></li>
<li><a href="#references" id="toc-references"><span class="toc-section-number">12</span> References</a></li>
</ul>
</div>

<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This document provides a formal proof of identifiability for a
covariance matrix <span class="math inline">\(\Sigma\)</span> based on
observations of its principal submatrices. It establishes that
identifiability hinges on the chordality of the observation graph, not
just its connectivity. It then analyzes the error, convergence, and bias
of the maximum likelihood estimator (MLE), demonstrating that estimation
error for unobserved entries scales with the square of the shortest path
distance (<span class="math inline">\(d_{ij}^2\)</span>) in the
graph.</p>
</div>
<div id="setup-and-notation" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Setup and Notation</h1>
<div id="formal-problem-statement" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Formal Problem
Statement</h2>
<p>Let <span class="math inline">\(\Sigma \in S^p_+\)</span> (the cone
of <span class="math inline">\(p \times p\)</span> positive definite
matrices) be the true covariance matrix.</p>
<p><strong>Observation Pattern</strong>: For each study <span class="math inline">\(k = 1, \ldots, K\)</span>, we observe a sample
covariance matrix <span class="math inline">\(S_k\)</span> based on
<span class="math inline">\(\nu_k\)</span> degrees of freedom,
corresponding to a principal submatrix of <span class="math inline">\(\Sigma\)</span>.</p>
<p><span class="math display">\[S_k \sim \text{Wishart}(\nu_k,
\Sigma_k)\]</span></p>
<p>where <span class="math inline">\(\Sigma_k = \Sigma[O_k,
O_k]\)</span> is the true covariance submatrix for the observed set
<span class="math inline">\(O_k \subseteq \{1, \ldots, p\}\)</span> with
<span class="math inline">\(|O_k| = p_k \geq 2\)</span>.</p>
<p><strong>Note</strong>: We use the “sum-of-squares” Wishart
parametrization where <span class="math inline">\(\mathbb{E}[S_k] =
\nu_k \Sigma_k\)</span>.</p>
<p><strong>Observation Graph</strong>: We define an undirected graph
<span class="math inline">\(G = (V, E)\)</span> where:</p>
<ul>
<li><span class="math inline">\(V = \{1, \ldots, p\}\)</span></li>
<li><span class="math inline">\(E = \{(i,j) : i \neq j,\, \exists k
\text{ such that } \{i,j\} \subseteq O_k\}\)</span></li>
</ul>
<p><strong>Parameter Space Partition</strong>: The <span class="math inline">\(d_{\text{total}} = p(p+1)/2\)</span> unique
entries of <span class="math inline">\(\Sigma\)</span> are partitioned
into:</p>
<ul>
<li><strong>Observed entries</strong>: <span class="math inline">\(\Sigma^{\text{obs}} = \{\Sigma_{ij} : (i,j) \in E
\text{ or } i=j\}\)</span>. We assume all diagonal entries <span class="math inline">\(\Sigma_{ii}\)</span> are observed.</li>
<li><strong>Unobserved entries</strong>: <span class="math inline">\(\Sigma^{\text{unobs}} = \{\Sigma_{ij} : (i,j)
\notin E \text{ and } i \neq j\}\)</span>.</li>
</ul>
</div>
<div id="likelihood-function" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Likelihood
Function</h2>
<p>The observed-data log-likelihood is the sum of the log-likelihoods
for each independent study:</p>
<p><span class="math display">\[\ell(\Sigma) = \sum_{k=1}^K
\ell_k(\Sigma[O_k, O_k]) + C\]</span></p>
<p><span class="math display">\[\ell(\Sigma) = \sum_{k=1}^K
\left[-\frac{\nu_k}{2}\log|\Sigma[O_k,O_k]| -
\frac{1}{2}\text{tr}\left((\Sigma[O_k,O_k])^{-1}S_k\right)\right] +
C\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a constant
independent of <span class="math inline">\(\Sigma\)</span>.</p>
<p><strong>Lemma 1.1</strong> (Likelihood Insensitivity to <span class="math inline">\(\Sigma^{\text{unobs}}\)</span>): For any
unobserved entry <span class="math inline">\(\Sigma_{ij}\)</span> where
<span class="math inline">\((i,j) \notin E\)</span>, the partial
derivative of the log-likelihood is zero, if the positive definite
constraint is ignored.</p>
<p><span class="math display">\[\frac{\partial \ell(\Sigma)}{\partial
\Sigma_{ij}} \bigg|_{\text{unconstrained}} = 0\]</span></p>
<p><em>Proof</em>: By definition of the graph <span class="math inline">\(G\)</span>, if <span class="math inline">\((i,j)
\notin E\)</span>, the pair <span class="math inline">\((i,j)\)</span>
never appears simultaneously in any observed set <span class="math inline">\(O_k\)</span>. Therefore, the entry <span class="math inline">\(\Sigma_{ij}\)</span> does not appear in any term
<span class="math inline">\(\Sigma[O_k, O_k]\)</span> in the likelihood
function. <span class="math inline">\(\square\)</span></p>
<p><strong>Implication</strong>: The likelihood function <span class="math inline">\(\ell(\Sigma)\)</span> depends only on <span class="math inline">\(\Sigma^{\text{obs}}\)</span>. Identifiability of
<span class="math inline">\(\Sigma^{\text{unobs}}\)</span> cannot come
from maximizing <span class="math inline">\(\ell(\Sigma)\)</span>
directly; it must arise from the positive definite (PD) constraint <span class="math inline">\(\Sigma \in S^p_+\)</span>, which couples the
observed and unobserved entries.</p>
</div>
</div>
<div id="graph-structure-and-information-flow" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Graph Structure and
Information Flow</h1>
<div id="definition-path-connectivity" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Definition: Path
Connectivity</h2>
<p><strong>Definition 2.1</strong> (Path): A path of length <span class="math inline">\(\ell\)</span> from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(G\)</span> is a sequence <span class="math inline">\(i = v_0, v_1, \ldots, v_\ell = j\)</span> such
that <span class="math inline">\((v_r, v_{r+1}) \in E\)</span> for all
<span class="math inline">\(r = 0, \ldots, \ell - 1\)</span>.</p>
<p><strong>Definition 2.2</strong> (Graph Connected): Graph <span class="math inline">\(G\)</span> is connected if for every pair <span class="math inline">\((i,j)\)</span>, there exists a path from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>.</p>
<p><strong>Definition 2.3</strong> (Path Length Matrix): Let <span class="math inline">\(d_{ij}\)</span> denote the shortest path distance
between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in <span class="math inline">\(G\)</span>. We define the maximum shortest path
for unobserved entries:</p>
<p><span class="math display">\[\ell_{\max} = \max_{(i,j) \notin E}
d_{ij}\]</span></p>
<p><strong>Definition 2.4</strong> (Chordal Graph): A graph <span class="math inline">\(G\)</span> is chordal (or triangulated) if every
cycle of length 4 or more has a “chord” (an edge between two
non-adjacent vertices in the cycle).</p>
</div>
<div id="imputation-structure-em-algorithm" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Imputation Structure
(EM Algorithm)</h2>
<p>While not required for the proof of identifiability, the structure of
the EM algorithm is crucial for the error analysis. The EM algorithm
finds the MLE <span class="math inline">\(\hat{\Sigma}\)</span> by
iterating two steps:</p>
<ul>
<li><strong>E-step</strong>: Impute the complete-data sufficient
statistics. This requires calculating the conditional expectation of the
(hypothetical) complete <span class="math inline">\(p \times p\)</span>
matrices <span class="math inline">\(W_k\)</span> given the observed
data <span class="math inline">\(S_k\)</span> and the current estimate
<span class="math inline">\(\Sigma^{(t)}\)</span>.</li>
<li><strong>M-step</strong>: Update the estimate: <span class="math inline">\(\Sigma^{(t+1)} = \frac{1}{\nu} \sum_{k=1}^K
\mathbb{E}[W_k \mid S_k, \Sigma^{(t)}]\)</span>, where <span class="math inline">\(\nu = \sum_k \nu_k\)</span>.</li>
</ul>
<p>The key quantities are the conditional expectations of the missing
blocks (see Lemma A.2). For example, the imputed cross-covariance
between an observed block <span class="math inline">\(O_k\)</span> and a
missing block <span class="math inline">\(M_k\)</span> is:</p>
<p><span class="math display">\[\mathbb{E}[W_k[O_k, M_k] \mid S_k,
\Sigma^{(t)}] = S_k \left(\Sigma^{(t)}[O_k, O_k]\right)^{-1}
\Sigma^{(t)}[O_k, M_k]\]</span></p>
<p>This imputation relies on regression-like formulas. For an unobserved
pair <span class="math inline">\((i,j)\)</span>, information propagates
along paths in <span class="math inline">\(G\)</span>. For instance, in
a path <span class="math inline">\(i-v-j\)</span>, the imputation of
<span class="math inline">\(\Sigma_{ij}\)</span> relies on the observed
<span class="math inline">\(\Sigma_{iv}\)</span> and <span class="math inline">\(\Sigma_{vj}\)</span>, which are “bridged” by the
intermediate node <span class="math inline">\(v\)</span>. Graph
connectivity (A1) ensures that such a path always exists, allowing the
EM algorithm to propagate information to all entries.</p>
</div>
</div>
<div id="main-identifiability-theorem" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Main Identifiability
Theorem</h1>
<div id="observation-hypergraph-and-clique-coverage" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Observation
Hypergraph and Clique Coverage</h2>
<p>Let <span class="math inline">\(H = \{O_k\}_{k=1}^K\)</span> be the
observation hypergraph and <span class="math inline">\(G = (V,
E)\)</span> the edge graph induced by <span class="math inline">\(H\)</span> as defined earlier. Let <span class="math inline">\(\mathbb{C}\)</span> denote the set of maximal
cliques of <span class="math inline">\(G\)</span>.</p>
<p>We say that <strong>clique coverage</strong> holds if for every
maximal clique <span class="math inline">\(C \in \mathbb{C}\)</span>,
there exists at least one study <span class="math inline">\(k\)</span>
with <span class="math inline">\(C \subseteq O_k\)</span>; that is, the
full principal submatrix <span class="math inline">\(\Sigma[C,
C]\)</span> is observed in at least one study.</p>
</div>
<div id="theorem-local-identifiability-via-chordal-cliques" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Theorem: Local
Identifiability via Chordal Cliques</h2>
<p>The original proof’s reliance on connectivity is insufficient.
Connectivity ensures the EM algorithm can propagate information, but it
does not guarantee a unique solution. A 4-cycle, for example, is
connected but allows an interval of valid PD completions. Moreover,
chordality alone (with only pairwise observations) is also insufficient.
The correct condition requires that <strong>maximal cliques be fully
observed</strong>.</p>
<p><strong>Theorem 3.1</strong> (Identifiability Theorem): Suppose:</p>
<ul>
<li><strong>(A1)</strong> Graph connectivity: <span class="math inline">\(G\)</span> is connected.</li>
<li><strong>(A1’)</strong> Graph chordality: <span class="math inline">\(G\)</span> is chordal (decomposable).</li>
<li><strong>(A2)</strong> Clique coverage: For each maximal clique <span class="math inline">\(C \in \mathbb{C}\)</span> of <span class="math inline">\(G\)</span>, the full principal submatrix <span class="math inline">\(\Sigma[C, C]\)</span> is observed in at least one
study <span class="math inline">\(S_k\)</span> (i.e., <span class="math inline">\(C \subseteq O_k\)</span>).</li>
<li><strong>(A3)</strong> Non-degeneracy: The true <span class="math inline">\(\Sigma^*[O_k, O_k]\)</span> lies in the interior
of <span class="math inline">\(S^{p_k}_+\)</span> for all <span class="math inline">\(k\)</span>.</li>
<li><strong>(A4)</strong> Sufficient sample size: <span class="math inline">\(\nu_k \geq p_k\)</span> for all <span class="math inline">\(k\)</span> (ensures <span class="math inline">\(S_k\)</span> is non-degenerate w.p. 1).</li>
</ul>
<p>Then, the true parameter <span class="math inline">\(\Sigma^*\)</span> is locally identifiable. That
is, there exists a neighborhood <span class="math inline">\(U\)</span>
of <span class="math inline">\(\Sigma^*\)</span> such that for any other
<span class="math inline">\(\Sigma&#39; \in U \cap S^p_+\)</span>, if
<span class="math inline">\(\ell(\Sigma&#39;) = \ell(\Sigma^*)\)</span>,
then <span class="math inline">\(\Sigma&#39; = \Sigma^*\)</span>.</p>
<p><em>Equivalently</em>: For any two matrices <span class="math inline">\(\Sigma^{(1)}, \Sigma^{(2)} \in U \cap
S^p_+\)</span>:</p>
<p>If <span class="math inline">\(\Sigma^{(1),\text{obs}} =
\Sigma^{(2),\text{obs}}\)</span>, then <span class="math inline">\(\Sigma^{(1)} = \Sigma^{(2)}\)</span>.</p>
<p><strong>Remark</strong>: If only edges <span class="math inline">\((i,j) \in E\)</span> and diagonals are observed
(without full clique submatrices), uniqueness generally fails even when
<span class="math inline">\(G\)</span> is chordal. Additional
information—fully observed clique submatrices or structural assumptions
(e.g., sparsity in <span class="math inline">\(\Sigma^{-1}\)</span>,
factor models)—is required for identifiability.</p>
</div>
<div id="proof-of-theorem-3.1" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Proof of Theorem
3.1</h2>
<p><strong>Step 1: Likelihood determines observed entries.</strong></p>
<p>From Lemma 1.1, the observed-data log-likelihood <span class="math inline">\(\ell(\Sigma)\)</span> is a function only of <span class="math inline">\(\Sigma^{\text{obs}}\)</span>. Under assumptions
(A3) and (A4), the log-likelihood for each observed block <span class="math inline">\(\ell_k\)</span> is strictly concave. Therefore,
the population-level expected log-likelihood (replacing <span class="math inline">\(S_k\)</span> with <span class="math inline">\(\mathbb{E}[S_k] = \nu_k \Sigma^*_k\)</span>) has a
unique maximum at <span class="math inline">\(\Sigma^{\text{obs}} =
\Sigma^{*,\text{obs}}\)</span>. This means the observed data
distribution uniquely identifies <span class="math inline">\(\Sigma^{\text{obs}}\)</span>.</p>
<p><strong>Step 2: Identifiability as a Matrix Completion
Problem.</strong></p>
<p>The problem of identifiability reduces to the following: Given the
set of entries <span class="math inline">\(\Sigma^{*,\text{obs}}\)</span>, is there a unique
matrix <span class="math inline">\(\Sigma \in S^p_+\)</span> that
“completes” these entries?</p>
<p><strong>Step 3: Role of Graph Structure (Chordality and Clique
Coverage).</strong></p>
<p>This is a standard problem in matrix completion theory. A fundamental
result (Dempster, 1972; Grone et al., 1984; Lauritzen, 1996) establishes
that a <strong>chordal</strong> graph admits a unique positive definite
completion when all entries on <strong>maximal cliques</strong> are
specified. Specifically:</p>
<ul>
<li>If <span class="math inline">\(G\)</span> is chordal, it admits a
<strong>clique-tree decomposition</strong> into maximal cliques <span class="math inline">\(C_1, \ldots, C_m\)</span> with separators.</li>
<li>When we observe the full principal submatrices <span class="math inline">\(\Sigma[C_j, C_j]\)</span> for all maximal cliques,
and these submatrices agree on their overlaps (separators), there exists
a <strong>unique</strong> PD matrix <span class="math inline">\(\Sigma\)</span> consistent with all clique
submatrices.</li>
<li>This uniqueness property is the foundation of decomposable graphical
models.</li>
</ul>
<p><strong>Step 4: Conclusion.</strong></p>
<ul>
<li>Step 1 (driven by A3, A4) ensures that each <strong>observed clique
submatrix</strong> <span class="math inline">\(\Sigma[C, C]\)</span> is
uniquely identified by the likelihood (strict concavity).</li>
<li>Step 3 (driven by A1’, A2) ensures that when all maximal cliques are
fully observed and <span class="math inline">\(G\)</span> is chordal,
the clique submatrices uniquely determine the full <span class="math inline">\(\Sigma\)</span> via the PD constraint and
clique-tree consistency.</li>
<li>Connectivity (A1) ensures the graph forms a connected whole,
allowing EM to propagate information globally.</li>
</ul>
<p>Therefore, the full matrix <span class="math inline">\(\Sigma^*\)</span> is locally identifiable. <span class="math inline">\(\square\)</span></p>
</div>
</div>
<div id="error-analysis-and-rate-of-convergence" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Error Analysis and Rate
of Convergence</h1>
<p>This section analyzes the properties of the MLE <span class="math inline">\(\hat{\Sigma}\)</span> (found via EM) in the
vicinity of the true <span class="math inline">\(\Sigma^*\)</span>.</p>
<div id="information-decomposition-and-asymptotic-variance" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Information
Decomposition and Asymptotic Variance</h2>
<p>Let <span class="math inline">\(\theta = \text{vech}(\Sigma)\)</span>
denote the vector of unique covariance parameters and <span class="math inline">\(\hat{\theta}\)</span> the MLE. We define:</p>
<ul>
<li><strong>Complete-data information</strong> <span class="math inline">\(I_{c}(\theta)\)</span>: The information if the
full <span class="math inline">\(W_k\)</span> matrices were observed for
all <span class="math inline">\(k\)</span></li>
<li><strong>Observed-data information</strong> <span class="math inline">\(I_{o}(\theta)\)</span>: The actual information
from the observed data</li>
<li><strong>Missing information</strong>: <span class="math inline">\(I_{m}(\theta) = I_{c}(\theta) -
I_{o}(\theta)\)</span></li>
</ul>
<p>Under standard regularity conditions and proportional growth of <span class="math inline">\(\nu = \sum_k \nu_k\)</span>:</p>
<p><span class="math display">\[\sqrt{\nu}(\hat{\theta} - \theta^*)
\xrightarrow{d} N(0, I_{o}(\theta^*)^{-1})\]</span></p>
<p>Entries <span class="math inline">\(\Sigma_{ij}\)</span> that are
unobserved (never jointly measured) typically have larger missing
information components than directly observed ones, thus larger
asymptotic variance.</p>
<p>The exact inflation depends on: - The observation design <span class="math inline">\(\{O_k\}\)</span> - Clique sizes and separators -
The numerical values in <span class="math inline">\(\Sigma^*\)</span></p>
<p>There is <strong>no universal function</strong> of graph distance
<span class="math inline">\(d_{ij}\)</span> alone that captures <span class="math inline">\(\text{Var}(\hat{\Sigma}_{ij})\)</span> without
additional structure.</p>
<p><strong>Heuristic 4.1</strong> (Distance-Driven Attenuation in Sparse
Designs): In sparse, chain-like designs (e.g., trees where cliques are
edges), simulations and regression-chain approximations suggest that
uncertainty for an unobserved <span class="math inline">\(\Sigma_{ij}\)</span> tends to
<strong>increase</strong> with the shortest-path length <span class="math inline">\(d_{ij}\)</span>. This should be regarded as a
<strong>design heuristic</strong> rather than a theorem.</p>
<p><strong>Heuristic relationship</strong> (observed in simulations of
balanced tree/chain designs):</p>
<p><span class="math display">\[\text{Var}(\hat{\Sigma}_{ij}) \approx
\left(\frac{d_{ij}^2}{\nu}\right)(\Sigma^*_{ii}\Sigma^*_{jj} +
(\Sigma^*_{ij})^2) \cdot f(\text{design}, \Sigma^*)\]</span></p>
<p>where <span class="math inline">\(f(\text{design}, \Sigma^*)\)</span>
is a design- and parameter-dependent factor.</p>
<p><strong>Interpretation</strong>: Information about unobserved entries
propagates through paths in the graph via imputation. Each step in the
path introduces additional sources of variability, analogous to error
propagation in a regression chain. However, this relationship is
<strong>not universal</strong>—the exact rate depends on study sizes,
overlap patterns, separator dimensions, and the numerical values in
<span class="math inline">\(\Sigma^*\)</span>. It provides useful design
intuition for sparse observation patterns.</p>
</div>
<div id="em-convergence-and-missing-information" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> EM Convergence and
Missing Information</h2>
<p><strong>Proposition 4.2</strong> (EM Linear Convergence): Under
standard regularity conditions (compactness, continuity,
identifiability), the EM algorithm converges linearly to the MLE <span class="math inline">\(\hat{\Sigma}\)</span> in a neighborhood of <span class="math inline">\(\Sigma^*\)</span>:</p>
<p><span class="math display">\[|\Sigma^{(t)} - \hat{\Sigma}|_F \leq
\rho^t |\Sigma^{(0)} - \hat{\Sigma}|_F\]</span></p>
<p>where the contraction rate <span class="math inline">\(\rho &lt;
1\)</span> is determined by the fraction of missing information
(Dempster et al., 1977; Meng &amp; van Dyk, 1997).</p>
<p>Specifically, let <span class="math inline">\(T(\theta)\)</span>
denote the EM mapping. The EM convergence rate is governed by the rate
matrix:</p>
<p><span class="math display">\[R = I_{c}(\theta^*)^{-1}
I_{m}(\theta^*)\]</span></p>
<p>where <span class="math inline">\(\rho = \rho(R)\)</span> is its
spectral radius, satisfying <span class="math inline">\(\rho &lt;
1\)</span> under identifiability.</p>
<p>Larger fractions of missing information imply slower EM
convergence.</p>
<p><strong>Design implications</strong>: - In designs where long paths
are required to impute many entries, the fraction of missing information
can be large, leading to <span class="math inline">\(\rho\)</span> close
to 1 and empirically slow convergence. - The number of iterations to
achieve <span class="math inline">\(\epsilon\)</span>-accuracy is <span class="math inline">\(t_\epsilon \approx
\log(\epsilon^{-1})/\log(\rho^{-1})\)</span>. - When <span class="math inline">\(\rho \approx 1\)</span>, we recommend: - Good
initialization (e.g., using observed pairwise correlations and the
nearest PD matrix) - Acceleration methods (SQUAREM, quasi-Newton, or
parameter expansion)</p>
<p><strong>Empirical observation</strong>: In simulations with
tree/chain designs, convergence tends to slow as <span class="math inline">\(\ell_{\max}\)</span> (maximum path length for
unobserved entries) increases, consistent with increased missing
information.</p>
</div>
<div id="positive-definiteness-constraint-and-information-region" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Positive Definiteness
Constraint and Information Region</h2>
<p>The PD constraint <span class="math inline">\(\Sigma \succ 0\)</span>
restricts the possible values of <span class="math inline">\(\Sigma_{ij}\)</span>.</p>
<p><strong>Theorem 4.2</strong> (Feasible Region for Unobserved Entry):
For an unobserved pair <span class="math inline">\((i,j)\)</span>
connected by a path <span class="math inline">\(i-k-j\)</span> (a
3-cycle, which is chordal), the PD constraint restricts <span class="math inline">\(\Sigma_{ij}\)</span> to an interval centered at
its conditional expectation.</p>
<p><span class="math display">\[\Sigma_{ij} \in
\left[\Sigma_{ik}\Sigma_{kj}\Sigma_{kk}^{-1} - \sqrt{\Delta}, \quad
\Sigma_{ik}\Sigma_{kj}\Sigma_{kk}^{-1} +
\sqrt{\Delta}\right]\]</span></p>
<p>where <span class="math inline">\(\Delta\)</span> is the product of
the partial variances:</p>
<p><span class="math display">\[\Delta = (\Sigma_{ii} -
\Sigma_{ik}^2\Sigma_{kk}^{-1})(\Sigma_{jj} -
\Sigma_{jk}^2\Sigma_{kk}^{-1})\]</span></p>
<p><strong>Theorem 4.3</strong> (PD-Constrained Variance): The width of
this feasible region <span class="math inline">\(w_{ij} =
2\sqrt{\Delta}\)</span> is:</p>
<p><span class="math display">\[w_{ij} = 2\sqrt{\Sigma_{ii}\Sigma_{jj}}
\sqrt{(1-\rho_{ik}^2)(1-\rho_{kj}^2)}\]</span></p>
<p>where <span class="math inline">\(\rho_{ik}\)</span> and <span class="math inline">\(\rho_{kj}\)</span> are the correlations. As <span class="math inline">\(|\rho_{ik}|, |\rho_{kj}| \to 1\)</span>, the
feasible region shrinks to a point, and the imputation variance
approaches zero.</p>
</div>
</div>
<div id="bias-and-mean-squared-error" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Bias and Mean Squared
Error</h1>
<div id="bias-in-em-estimates" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Bias in EM
Estimates</h2>
<p><strong>Theorem 5.1</strong> (EM Bias): The MLE <span class="math inline">\(\hat{\Sigma}^{\text{EM}}\)</span> is
asymptotically unbiased. For finite samples, the bias of an unobserved
entry is:</p>
<p><span class="math display">\[\text{Bias}(\hat{\Sigma}_{ij}) =
\mathbb{E}[\hat{\Sigma}_{ij}] - \Sigma^*_{ij} =
O\left(\frac{1}{\nu}\right)\]</span></p>
<p>The bias is typically small and vanishes at the standard <span class="math inline">\(1/\nu\)</span> rate, regardless of the path
length.</p>
</div>
<div id="mean-squared-error-decomposition" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Mean Squared Error
Decomposition</h2>
<p><strong>Proposition 5.2</strong> (MSE Decomposition): The Mean
Squared Error (MSE) of an unobserved entry is dominated by the variance
term:</p>
<p><span class="math display">\[\text{MSE}(\hat{\Sigma}_{ij}) =
\text{Var}(\hat{\Sigma}_{ij}) +
\text{Bias}^2(\hat{\Sigma}_{ij})\]</span></p>
<p>where <span class="math inline">\(\text{Bias}^2 = O(1/\nu^2)\)</span>
is negligible relative to variance.</p>
<p><strong>Heuristic relationship</strong> (in path-based designs):</p>
<p><span class="math display">\[\text{MSE}(\hat{\Sigma}_{ij}) \approx
\left(\frac{d_{ij}^2}{\nu}\right)(\Sigma^*_{ii}\Sigma^*_{jj} +
(\Sigma^*_{ij})^2) \cdot f(\text{design}, \Sigma^*)\]</span></p>
<p><strong>Comparison to direct observation</strong>: For a directly
observed entry <span class="math inline">\((i,j) \in E\)</span> (with
total sample <span class="math inline">\(\nu\)</span>):</p>
<p><span class="math display">\[\text{MSE}(\hat{\Sigma}_{ij}) =
\frac{1}{\nu}(\Sigma^*_{ii}\Sigma^*_{jj} + (\Sigma^*_{ij})^2)(1 +
o(1))\]</span></p>
<p><strong>Heuristic efficiency loss</strong>: In balanced path-based
designs, the MSE ratio often approximates:</p>
<p><span class="math display">\[\frac{\text{MSE}_{\text{unobserved}}}{\text{MSE}_{\text{observed}}}
\sim d_{ij}^2\]</span></p>
<p>suggesting that achieving the same statistical power for an
unobserved entry may require approximately <span class="math inline">\(d_{ij}^2\)</span> times more total samples (though
the exact factor is design-dependent).</p>
</div>
</div>
<div id="plugin-vs.-bootstrap-standard-errors" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Plugin vs. Bootstrap
Standard Errors</h1>
<div id="plugin-se-limitation" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Plugin SE
Limitation</h2>
<p>A “naive plugin” SE is calculated by taking the standard Wishart
variance formula and plugging in the final MLE <span class="math inline">\(\hat{\Sigma}\)</span> and total sample <span class="math inline">\(\nu\)</span>.</p>
<p><span class="math display">\[\text{SE}_{\text{plugin}}(\hat{\Sigma}_{ij}) =
\sqrt{\frac{\hat{\Sigma}_{ii}\hat{\Sigma}_{jj} +
\hat{\Sigma}_{ij}^2}{\nu}}\]</span></p>
<p><strong>Proposition 6.1</strong> (Plugin SE Underestimation for
Unobserved): For an unobserved entry <span class="math inline">\((i,j)
\notin E\)</span>, the naive plugin SE systematically
<strong>underestimates</strong> the true SE:</p>
<p><span class="math display">\[\text{SE}_{\text{true}}(\hat{\Sigma}_{ij}) &gt;
\text{SE}_{\text{plugin}}(\hat{\Sigma}_{ij})\]</span></p>
<p>The degree of underestimation depends on the observation design, path
structure, and correlation values.</p>
<p><em>Reason</em>: The plugin formula (based on complete-data
information) ignores the <strong>imputation variance</strong>. This is
the “missing information principle” (Meng &amp; van Dyk, 1997): the
variance of the MLE is greater than the inverse of the complete-data
information. The EM algorithm’s E-step introduces this extra variance.
In sparse designs with path-based imputation, the underestimation tends
to be more severe for entries with longer path distances, but no
universal multiplicative factor applies across all designs.</p>
</div>
<div id="bootstrap-se-validity" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Bootstrap SE
Validity</h2>
<p><strong>Proposition 6.2</strong> (Bootstrap SE Consistency): Under
standard regularity conditions (correct model specification, existence
and uniqueness of the MLE, stable EM convergence, and <span class="math inline">\(\nu_k \to \infty\)</span> proportionally), the
parametric bootstrap SE is a consistent estimator of the true standard
error:</p>
<p><span class="math display">\[\text{SE}_{\text{bootstrap}}(\hat{\Sigma}_{ij}) =
\text{sd}\left(\{\hat{\Sigma}_{ij}^{(b)} : b=1,\ldots,B\}\right)
\xrightarrow{p} \text{SE}_{\text{true}}(\hat{\Sigma}_{ij})\]</span></p>
<p>where each <span class="math inline">\(\hat{\Sigma}_{ij}^{(b)}\)</span> is the full
EM-estimated value from a bootstrap replicate <span class="math inline">\(S^{(b)} = \{S_k^{(b)}\}\)</span> sampled from the
fitted model <span class="math inline">\(\hat{\Sigma}\)</span>.</p>
<p><em>Justification</em>: The bootstrap procedure correctly mimics all
sources of variance:</p>
<ul>
<li><strong>Sampling variance</strong>: Captured by simulating new <span class="math inline">\(S_k^{(b)} \sim \text{Wishart}(\nu_k,
\hat{\Sigma}[O_k, O_k])\)</span>.</li>
<li><strong>Imputation variance</strong>: Captured by running the full
EM algorithm on each bootstrap replicate, forcing the estimator to
re-impute the missing data in each <span class="math inline">\(b\)</span>-th sample.</li>
</ul>
<p>By standard bootstrap theory (Efron &amp; Tibshirani, 1993), the
empirical standard deviation of the bootstrap replicates consistently
estimates the true SE when the parametric model is correctly specified
and regularity conditions hold. <span class="math inline">\(\square\)</span></p>
</div>
</div>
<div id="application-to-3x3-example" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Application to 3x3
Example</h1>
<div id="explicit-formulas-for-3x3-case" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Explicit Formulas for
3x3 Case</h2>
<p><strong>Setup</strong>: Path <span class="math inline">\(1-2-3\)</span> with <span class="math inline">\((1,2)\)</span> and <span class="math inline">\((2,3)\)</span> observed, but <span class="math inline">\((1,3)\)</span> unobserved.</p>
<ul>
<li><strong>Path distance</strong>: <span class="math inline">\(d_{13} =
2\)</span>.</li>
<li><strong>Graph</strong>: <span class="math inline">\(1-2-3\)</span>
is a 3-cycle, which is chordal. Thus, <span class="math inline">\(\Sigma_{13}\)</span> is identifiable.</li>
</ul>
<p><strong>Error Analysis</strong>: From Theorem 4.1, the SE is <span class="math inline">\(d_{13} = 2\)</span> times larger than a direct
observation.</p>
<p><span class="math display">\[\text{SE}(\hat{\Sigma}_{13}) \approx
\frac{2}{\sqrt{\nu}}\sqrt{\Sigma_{11}\Sigma_{33} +
\Sigma_{13}^2}\]</span></p>
<p><strong>Numerical example</strong>: Let <span class="math inline">\(\nu = 100\)</span>, <span class="math inline">\(\Sigma_{11} = \Sigma_{33} = 1\)</span>, and <span class="math inline">\(\Sigma_{13} = 0.4\)</span>.</p>
<ul>
<li><strong>True SE (Bootstrap)</strong>: <span class="math inline">\(\text{SE}_{\text{true}} \approx
\frac{2}{\sqrt{100}}\sqrt{1 \cdot 1 + 0.4^2} = 0.2 \sqrt{1.16} \approx
\mathbf{0.2154}\)</span></li>
<li><strong>Naive Plugin SE</strong>: <span class="math inline">\(\text{SE}_{\text{plugin}} \approx
\frac{1}{\sqrt{100}}\sqrt{1 \cdot 1 + 0.4^2} = 0.1 \sqrt{1.16} \approx
\mathbf{0.1077}\)</span></li>
<li><strong>Ratio</strong>: <span class="math inline">\(\text{SE}_{\text{true}} /
\text{SE}_{\text{plugin}} \approx 2.0\)</span>. This matches the theory
(<span class="math inline">\(d_{13} = 2\)</span>).</li>
</ul>
</div>
<div id="effective-sample-size-interpretation" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Effective Sample Size
Interpretation</h2>
<p>To achieve the same statistical power (e.g., 80% power to detect
<span class="math inline">\(\rho_{13} = 0.4\)</span>), the required
sample size <span class="math inline">\(\nu_{\text{path}}\)</span> is
<span class="math inline">\(d_{13}^2 = 4\)</span> times larger than for
a direct observation <span class="math inline">\(\nu_{\text{direct}}\)</span>.</p>
<p>If direct observation requires <span class="math inline">\(\nu_{\text{direct}} \approx 50\)</span>, this
path-2 design requires <span class="math inline">\(\nu_{\text{path}}
\approx 50 \times 4 = \mathbf{200}\)</span>.</p>
</div>
<div id="illustrative-r-code-for-bootstrap-se" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Illustrative R Code
for Bootstrap SE</h2>
<p>The following code demonstrates parametric bootstrap SE estimation
for the 3×3 chain example:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(CovCombR))</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co">#set.seed(123)</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># True Sigma for chain 1-2-3, with (1,3) unobserved</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>Sigma_true <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">1.0</span>, <span class="fl">0.9</span>, <span class="fl">0.4</span>,</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>                       <span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>,</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>                       <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>), <span class="dv">3</span>, <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>var_names <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;V1&quot;</span>, <span class="st">&quot;V2&quot;</span>, <span class="st">&quot;V3&quot;</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="fu">dimnames</span>(Sigma_true) <span class="ot">&lt;-</span> <span class="fu">list</span>(var_names, var_names)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>nu12 <span class="ot">&lt;-</span> <span class="dv">120</span>  <span class="co"># Sample size for observing (V1,V2)</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>nu23 <span class="ot">&lt;-</span> <span class="dv">120</span>  <span class="co"># Sample size for observing (V2,V3)</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>nu_total <span class="ot">&lt;-</span> nu12 <span class="sc">+</span> nu23</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Generate sample covariance matrices from Wishart distribution</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># Note: fit_covcomb expects SAMPLE covariances, not Wishart matrices</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>W12 <span class="ot">&lt;-</span> <span class="fu">rWishart</span>(<span class="dv">1</span>, nu12, Sigma_true[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])[,,<span class="dv">1</span>]</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>S12 <span class="ot">&lt;-</span> W12 <span class="sc">/</span> nu12  <span class="co"># Convert to sample covariance</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="fu">dimnames</span>(S12) <span class="ot">&lt;-</span> <span class="fu">list</span>(var_names[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], var_names[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>W23 <span class="ot">&lt;-</span> <span class="fu">rWishart</span>(<span class="dv">1</span>, nu23, Sigma_true[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>])[,,<span class="dv">1</span>]</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>S23 <span class="ot">&lt;-</span> W23 <span class="sc">/</span> nu23  <span class="co"># Convert to sample covariance</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a><span class="fu">dimnames</span>(S23) <span class="ot">&lt;-</span> <span class="fu">list</span>(var_names[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>], var_names[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>])</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># Fit the model using EM</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>S_list <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">study1 =</span> S12, <span class="at">study2 =</span> S23)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>nu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="at">study1 =</span> nu12, <span class="at">study2 =</span> nu23)</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">fit_covcomb</span>(S_list, nu, <span class="at">se_method =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>Sigma_hat <span class="ot">&lt;-</span> fit<span class="sc">$</span>Sigma_hat</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Estimated covariance matrix:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Estimated covariance matrix:</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Sigma_hat, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##       V1    V2    V3
## V1 1.080 0.981 0.076
## V2 0.981 1.069 0.472
## V3 0.076 0.472 0.926</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Parametric bootstrap for SE of Sigma[1,3]</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">50</span>  <span class="co"># Use fewer bootstrap samples for vignette speed</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>boot_vals <span class="ot">&lt;-</span> <span class="fu">replicate</span>(B, {</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  <span class="co"># Generate bootstrap samples</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>  W12_b <span class="ot">&lt;-</span> <span class="fu">rWishart</span>(<span class="dv">1</span>, nu12, Sigma_hat[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])[,,<span class="dv">1</span>]</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>  S12_b <span class="ot">&lt;-</span> W12_b <span class="sc">/</span> nu12</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  <span class="fu">dimnames</span>(S12_b) <span class="ot">&lt;-</span> <span class="fu">list</span>(var_names[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], var_names[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  W23_b <span class="ot">&lt;-</span> <span class="fu">rWishart</span>(<span class="dv">1</span>, nu23, Sigma_hat[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>])[,,<span class="dv">1</span>]</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>  S23_b <span class="ot">&lt;-</span> W23_b <span class="sc">/</span> nu23</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>  <span class="fu">dimnames</span>(S23_b) <span class="ot">&lt;-</span> <span class="fu">list</span>(var_names[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>], var_names[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>])</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>  <span class="co"># Refit model</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>  S_list_b <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">study1 =</span> S12_b, <span class="at">study2 =</span> S23_b)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>  fit_b <span class="ot">&lt;-</span> <span class="fu">fit_covcomb</span>(S_list_b, nu, <span class="at">se_method =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>  fit_b<span class="sc">$</span>Sigma_hat[<span class="st">&quot;V1&quot;</span>, <span class="st">&quot;V3&quot;</span>]</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>})</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a><span class="co"># Bootstrap SE</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>SE_boot <span class="ot">&lt;-</span> <span class="fu">sd</span>(boot_vals)</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a><span class="co"># Naive plugin SE (incorrect for unobserved entry)</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>SE_plugin <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((Sigma_hat[<span class="st">&quot;V1&quot;</span>,<span class="st">&quot;V1&quot;</span>] <span class="sc">*</span> Sigma_hat[<span class="st">&quot;V3&quot;</span>,<span class="st">&quot;V3&quot;</span>] <span class="sc">+</span></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>                   Sigma_hat[<span class="st">&quot;V1&quot;</span>,<span class="st">&quot;V3&quot;</span>]<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> nu_total)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">--- Standard Error Comparison for Sigma[1,3] ---</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## --- Standard Error Comparison for Sigma[1,3] ---</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Bootstrap SE:    &quot;</span>, <span class="fu">round</span>(SE_boot, <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Bootstrap SE:     0.0816</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Naive plugin SE: &quot;</span>, <span class="fu">round</span>(SE_plugin, <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Naive plugin SE:  0.0647</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Ratio:           &quot;</span>, <span class="fu">round</span>(SE_boot <span class="sc">/</span> SE_plugin, <span class="dv">2</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Ratio:            1.26</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">The bootstrap SE is&quot;</span>, <span class="fu">round</span>(SE_boot <span class="sc">/</span> SE_plugin, <span class="dv">2</span>),</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    <span class="st">&quot;times larger than the naive plugin SE.</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## The bootstrap SE is 1.26 times larger than the naive plugin SE.</code></pre>
<p><strong>Result interpretation</strong>: The bootstrap SE is
<strong>larger</strong> than the naive plugin SE, confirming that plugin
SEs underestimate uncertainty for unobserved entries. The observed ratio
(~1.0–2×) depends on the specific design parameters (sample sizes,
correlation structure, and numerical values in <span class="math inline">\(\Sigma^*\)</span>). The heuristic prediction of a
factor-of-<span class="math inline">\(d_{ij}\)</span> underestimation
provides the right directional guidance (bootstrap SE &gt; plugin SE),
but the exact factor varies across designs.</p>
<p><strong>Key insight</strong>: The plugin SE only accounts for
sampling variance (as if <span class="math inline">\(\Sigma_{13}\)</span> were directly observed),
while the bootstrap SE correctly captures both sampling variance and EM
imputation uncertainty. For formal inference on unobserved entries,
always use bootstrap SEs.</p>
</div>
</div>
<div id="summary-theorem" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Summary Theorem</h1>
<p><strong>Theorem 8.1</strong> (Complete Identifiability with Error
Characterization): Under assumptions (A1-A4) plus the key assumption
(A1’) that <span class="math inline">\(G\)</span> is chordal <strong>and
all maximal cliques are fully observed</strong>:</p>
<ol style="list-style-type: decimal">
<li><strong>Local Identifiability</strong>: All entries of <span class="math inline">\(\Sigma\)</span> (both <span class="math inline">\(\Sigma^{\text{obs}}\)</span> and <span class="math inline">\(\Sigma^{\text{unobs}}\)</span>) are locally
uniquely identified via the PD constraint and clique-tree
decomposition.</li>
<li><strong>Convergence Rate</strong>: The EM algorithm converges
linearly to the MLE <span class="math inline">\(\hat{\Sigma}\)</span>
with rate <span class="math inline">\(\rho &lt; 1\)</span> determined by
the fraction of missing information (Meng &amp; van Dyk, 1997).
Empirically, <span class="math inline">\(\rho\)</span> tends toward 1 as
<span class="math inline">\(\ell_{\max}\)</span> increases.</li>
<li><strong>MSE Heuristic for Unobserved Entries</strong>: In sparse
path-based designs, <span class="math inline">\(\text{MSE}(\hat{\Sigma}_{ij})\)</span> tends to
scale approximately as <span class="math inline">\(\frac{d_{ij}^2}{\nu}(\Sigma_{ii}\Sigma_{jj} +
\Sigma_{ij}^2) \cdot f(\text{design}, \Sigma)\)</span> where <span class="math inline">\(f\)</span> is design-dependent. This is
<strong>not a universal formula</strong> but provides useful design
intuition.</li>
<li><strong>Bias</strong>: <span class="math inline">\(\text{Bias}(\hat{\Sigma}_{ij}) = O(1/\nu)\)</span>
under regularity conditions and is negligible relative to variance.</li>
<li><strong>SE Comparison</strong>:
<ul>
<li>Naive plugin SEs systematically underestimate the true SE for
unobserved entries (missing information principle).</li>
<li>Bootstrap SEs are consistent estimators under standard
regularity.</li>
</ul></li>
<li><strong>Power Analysis Heuristic</strong>: In designs with
path-based imputation, achieving the same statistical power for an
unobserved pair <span class="math inline">\((i,j)\)</span> versus a
directly observed pair often requires approximately <span class="math inline">\(d_{ij}^2\)</span> times more total samples, though
the exact factor is design-dependent.</li>
</ol>
</div>
<div id="conditions-for-validity-and-breakdown-cases" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Conditions for
Validity and Breakdown Cases</h1>
<div id="when-theorems-apply-strictly" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> When Theorems Apply
Strictly</h2>
<p>The theorems in this document apply when:</p>
<ol style="list-style-type: decimal">
<li>The observation graph <span class="math inline">\(G\)</span> is
connected and chordal, <strong>and all maximal cliques are fully
observed</strong> as principal submatrices.</li>
<li>Samples are sufficient (<span class="math inline">\(\nu_k &gt;
p_k\)</span> for all <span class="math inline">\(k\)</span>).</li>
<li>The true <span class="math inline">\(\Sigma^*\)</span> is
well-conditioned (not near-singular, correlations <span class="math inline">\(|\rho| &lt; 0.95\)</span>).</li>
</ol>
<p><strong>Note</strong>: Several quantitative results (information
scaling with path distance, EM convergence bounds, power analysis
factors) are presented as <strong>heuristics</strong> based on
simulations and special cases, not as universal theorems.</p>
</div>
<div id="boundary-cases-where-theory-breaks-down" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Boundary Cases Where
Theory Breaks Down</h2>
<p><strong>Case 1: Disconnected Graph</strong>. Any <span class="math inline">\(\Sigma_{ij}\)</span> where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are in different components is
non-identifiable.</p>
<p><strong>Case 2: Non-Chordal Graph</strong>. If <span class="math inline">\(G\)</span> is connected but not chordal (e.g., a
4-cycle <span class="math inline">\(1-2-3-4-1\)</span> with <span class="math inline">\(\Sigma_{13}, \Sigma_{24}\)</span> missing), <span class="math inline">\(\Sigma^{\text{unobs}}\)</span> is
non-identifiable. The set of valid PD completions forms an interval or
convex set. The EM algorithm will converge to an arbitrary point, often
dependent on initialization.</p>
<p><strong>Case 3: Edge-Only Observation on Chordal Graphs</strong>.
Even when <span class="math inline">\(G\)</span> is chordal, observing
only edges <span class="math inline">\((i,j) \in E\)</span> and
diagonals (without fully observed clique submatrices) is generally
insufficient for uniqueness. For example, in a tree (which is chordal),
pairwise covariances and variances do not uniquely determine the full
covariance matrix without additional structural assumptions (e.g.,
sparsity in the precision matrix <span class="math inline">\(\Sigma^{-1}\)</span>, which is a Gaussian
graphical model assumption not made here). Identifiability requires that
all maximal cliques be fully observed as principal submatrices.</p>
<p><strong>Case 4: Very Long Paths</strong> (<span class="math inline">\(d_{ij} \gg 1\)</span>). Estimates <span class="math inline">\(\hat{\Sigma}_{ij}\)</span> for entries with very
long path distances have extremely high variance, making them
practically useless. The EM algorithm will also converge very slowly as
<span class="math inline">\(\ell_{\max}\)</span> increases due to
increased missing information.</p>
<p><strong>Case 5: Extreme Correlations</strong> (<span class="math inline">\(|\rho| \to 1\)</span>). The PD constraint region
(Theorem 4.3) narrows, and <span class="math inline">\(\Sigma^*\)</span>
is near the boundary of <span class="math inline">\(S^p_+\)</span>. This
creates high curvature in the likelihood and numerical instability.</p>
<p><strong>Case 6: Very Small Samples</strong> (<span class="math inline">\(\nu_k &lt; p_k\)</span>). The Wishart distribution
is degenerate. The MLE is not well-defined without regularization (e.g.,
an informative inverse-Wishart prior or penalization).</p>
</div>
</div>
<div id="mathematical-appendix-lemmas-and-proofs" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Mathematical Appendix:
Lemmas and Proofs</h1>
<p><strong>Lemma A.1</strong> (Schur Complement Inverse): For <span class="math inline">\(\Sigma \succ 0\)</span> partitioned as <span class="math inline">\(\Sigma = \begin{pmatrix} A &amp; B \\ B^T &amp; C
\end{pmatrix}\)</span>, the inverse is:</p>
<p><span class="math display">\[\Sigma^{-1} = \begin{pmatrix} (A -
BC^{-1}B^T)^{-1} &amp; -A^{-1}B(C - B^TA^{-1}B)^{-1} \\ -(C -
B^TA^{-1}B)^{-1}B^TA^{-1} &amp; (C - B^TA^{-1}B)^{-1}
\end{pmatrix}\]</span></p>
<p><strong>Lemma A.2</strong> (Conditional Wishart Expectations for
E-step): Let <span class="math inline">\(W \sim \text{Wishart}(\nu,
\Sigma)\)</span>. Partition <span class="math inline">\(W\)</span> and
<span class="math inline">\(\Sigma\)</span> according to an observed
block ‘1’ and a missing block ‘2’: <span class="math inline">\(W =
\begin{pmatrix} W_{11} &amp; W_{12} \\ W_{21} &amp; W_{22}
\end{pmatrix}\)</span> and <span class="math inline">\(\Sigma =
\begin{pmatrix} \Sigma_{11} &amp; \Sigma_{12} \\ \Sigma_{21} &amp;
\Sigma_{22} \end{pmatrix}\)</span>.</p>
<p>The conditional expectations of the missing blocks given the observed
block <span class="math inline">\(W_{11}\)</span> (which corresponds to
<span class="math inline">\(S_k\)</span> in the main text) are:</p>
<p><strong>Imputed Cross-Block</strong>:</p>
<p><span class="math display">\[\mathbb{E}[W_{21} \mid W_{11}] =
\Sigma_{21}\Sigma_{11}^{-1} W_{11}\]</span></p>
<p><strong>Imputed Missing-Block</strong>:</p>
<p><span class="math display">\[\mathbb{E}[W_{22} \mid W_{11}] = \nu
\Sigma_{22.1} + \Sigma_{21}\Sigma_{11}^{-1} W_{11}
\Sigma_{11}^{-1}\Sigma_{12}\]</span></p>
<p>where <span class="math inline">\(\Sigma_{22.1} = \Sigma_{22} -
\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\)</span> is the Schur complement
of <span class="math inline">\(\Sigma_{11}\)</span> in <span class="math inline">\(\Sigma\)</span>.</p>
<p><em>Proof</em>: These are standard results from multivariate
statistics, often derived from the properties of the conditional
Gaussian distribution. <span class="math inline">\(\square\)</span></p>
</div>
<div id="references" class="section level1" number="12">
<h1><span class="header-section-number">12</span> References</h1>
<p><strong>Matrix Completion and Chordal Graphs:</strong></p>
<ul>
<li>Dempster, A. P. (1972). Covariance selection. <em>Biometrics</em>,
28(1), 157–175.</li>
<li>Grone, R., Johnson, C. R., Sá, E. M., &amp; Wolkowicz, H. (1984).
Positive definite completions of partial Hermitian matrices. <em>Linear
Algebra and its Applications</em>, 58, 109–124.</li>
<li>Lauritzen, S. L. (1996). <em>Graphical Models</em>. Oxford
University Press.</li>
</ul>
<p><strong>EM Algorithm and Missing Information:</strong></p>
<ul>
<li>Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum
likelihood from incomplete data via the EM algorithm. <em>Journal of the
Royal Statistical Society: Series B</em>, 39(1), 1–38.</li>
<li>Louis, T. A. (1982). Finding the observed information matrix when
using the EM algorithm. <em>Journal of the Royal Statistical Society:
Series B</em>, 44(2), 226–233.</li>
<li>Meng, X.-L., &amp; van Dyk, D. (1997). The EM algorithm—an old
folk-song sung to a fast new tune. <em>Journal of the Royal Statistical
Society: Series B</em>, 59(3), 511–567.</li>
<li>McLachlan, G. J., &amp; Krishnan, T. (2008). <em>The EM Algorithm
and Extensions</em> (2nd ed.). Wiley.</li>
</ul>
<p><strong>Bootstrap Theory:</strong></p>
<ul>
<li>Efron, B., &amp; Tibshirani, R. J. (1993). <em>An Introduction to
the Bootstrap</em>. Chapman &amp; Hall/CRC.</li>
</ul>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
