---
title: "Standard Errors and Statistical Inference"
author: "Deniz Akdemir"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Standard Errors and Statistical Inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
library(CovCombR)
```

## Overview

Standard errors (SEs) quantify the uncertainty in your covariance estimates. CovCombR offers three methods for computing standard errors, each with distinct trade-offs between speed, accuracy, and reliability:

| Method | Speed | Reliability | Best For |
|--------|-------|-------------|----------|
| **plugin** | Fast (seconds) | Anti-conservative | Quick diagnostics |
| **bootstrap** | Slow (minutes-hours) | Most reliable | Publication, inference |
| **sem** | Medium (seconds-minutes) | Experimental | Large problems (validate!) |

**Key insight:** For entries never jointly observed in any study, only bootstrap provides valid uncertainty quantification. Plugin SEs ignore EM imputation variance and will be too small.

---

## Which Method Should I Use?

### Decision Tree

```
Do you need standard errors at all?
├─ NO → use se_method = "none" (fastest)
└─ YES
   ├─ Quick diagnostic / exploratory analysis?
   │  └─ YES → use se_method = "plugin"
   │     ⚠️  WARNING: Anti-conservative for imputed entries
   │
   └─ Publication / formal inference?
      ├─ Small-medium problem (p < 50, reasonable runtime)?
      │  └─ YES → use se_method = "bootstrap" (RECOMMENDED)
      │
      └─ Large problem (p > 50, bootstrap too slow)?
         └─ YES → use se_method = "sem"
            ⚠️  EXPERIMENTAL: Validate against bootstrap on subset
```

### Quick Reference

**Use plugin** when:
- You need quick diagnostics
- Sample sizes are large (ν >> p)
- Most variable pairs are directly observed
- You're exploring models, not publishing results

**Use bootstrap** when:
- Formal hypothesis testing
- Confidence intervals for publication
- Entries have substantial missing data
- You need valid inference (worth the wait)

**Use SEM** when:
- Bootstrap is prohibitively slow (p > 100)
- You have computational constraints
- You can validate results against bootstrap on a subset
- You're willing to check diagnostics carefully

---

## Method 1: Plugin Standard Errors

### Theory

Plugin SEs use the observed Fisher information matrix, assuming the Wishart variance formula applies directly:

$$
\text{Var}(\hat{\Sigma}_{ij}) \approx \frac{1}{\sum_k \nu_k \cdot \mathbb{1}(i,j \in \mathcal{O}_k)}
\left( \Sigma_{ij}^2 + \Sigma_{ii}\Sigma_{jj} \right)
$$

where the sum is over studies observing both variables i and j.

### Limitations

**CRITICAL:** Plugin SEs are **anti-conservative** for imputed entries because they:
1. **Ignore EM imputation uncertainty** - assume complete data
2. **Underestimate by factor d²ij** where dij = graph distance between variables i,j
3. **Return NA for never-observed pairs** (honest about complete ignorance)

### When Plugin Works Well

- Large sample sizes (ν > 100 per study)
- High overlap (most pairs directly observed)
- Quick model comparison (relative magnitudes)

### Example: Plugin SEs on Iris Data

```{r plugin_example}
# Simulate 3 studies with overlapping Iris variables
set.seed(2025)
data(iris)

# Study 1: Sepal measurements (vars 1-2)
S1 <- cov(iris[1:50, 1:2])

# Study 2: Petal measurements (vars 3-4)
S2 <- cov(iris[51:100, 3:4])

# Study 3: Sepal.Length + Petal.Length (vars 1,3)
S3 <- cov(iris[101:150, c(1,3)])

# Fit with plugin SEs
fit_plugin <- fit_covcomb(
  S_list = list(Study1 = S1, Study2 = S2, Study3 = S3),
  nu = c(Study1 = 49, Study2 = 49, Study3 = 49),
  se_method = "plugin"
)

# Extract results
Sigma_hat <- coef(fit_plugin)
Sigma_se <- fit_plugin$Sigma_se

# Show SEs for directly observed vs imputed pairs
cat("SE for Sepal.Length--Sepal.Width (directly observed in Study 1):\n")
cat(sprintf("  Estimate: %.3f, SE: %.3f\n", Sigma_hat[1,2], Sigma_se[1,2]))

cat("\nSE for Sepal.Width--Petal.Length (never jointly observed):\n")
cat(sprintf("  Estimate: %.3f, SE: %s\n", Sigma_hat[2,3],
    ifelse(is.na(Sigma_se[2,3]), "NA (not observed together)",
           sprintf("%.3f", Sigma_se[2,3]))))
```

**Interpretation:** Plugin honestly returns `NA` for pairs never observed together, acknowledging complete uncertainty.

---

## Method 2: Bootstrap Standard Errors (RECOMMENDED)

### Theory

Parametric bootstrap:
1. Simulate B datasets from fitted model: $\mathbf{W}_k^{(b)} \sim \text{Wishart}(\nu_k, \hat{\alpha}_k \hat{\Sigma})$
2. Refit model on each simulated dataset
3. Compute empirical standard deviation: $\text{SE}(\hat{\Sigma}_{ij}) = \text{SD}_b(\hat{\Sigma}_{ij}^{(b)})$

### Why Bootstrap is Correct

Bootstrap **properly accounts for**:
- ✓ Sampling variance (like plugin)
- ✓ EM imputation uncertainty (plugin ignores this!)
- ✓ Non-normal distributions of estimates
- ✓ Correlation between entries

### Configuration

```{r bootstrap_config, eval=FALSE}
fit_boot <- fit_covcomb(
  S_list, nu,
  se_method = "bootstrap",
  control = list(
    bootstrap = list(
      B = 200,           # Number of replicates (default)
      seed = 2025,       # Reproducibility
      progress = TRUE,   # Show progress bar
      verbose = FALSE    # Suppress warnings for failed fits
    )
  )
)
```

**Choosing B:**
- B = 100: Quick check (~5 min for p=20)
- B = 200: Standard (default, recommended)
- B = 500: High precision for critical applications
- B = 1000: Overkill for most purposes

### Example: Bootstrap Inference

```{r bootstrap_example}
# Same iris setup as before
set.seed(2025)
data(iris)
S_list <- list(
  Study1 = cov(iris[1:50, 1:2]),
  Study2 = cov(iris[51:100, 3:4]),
  Study3 = cov(iris[101:150, c(1,3)])
)
nu_vec <- c(Study1 = 49, Study2 = 49, Study3 = 49)

# Fit with bootstrap SEs (small B for vignette speed)
fit_boot <- fit_covcomb(
  S_list, nu_vec,
  se_method = "bootstrap",
  control = list(bootstrap = list(B = 50, seed = 2025, progress = FALSE))
)

# Hypothesis test: Is Sepal.Length--Petal.Length correlation non-zero?
# (These were NEVER observed together in same study!)
Sigma_hat <- coef(fit_boot)
Sigma_se <- fit_boot$Sigma_se

idx_sepal <- 1  # Sepal.Length
idx_petal <- 2  # Petal.Length (in Study3 this was variable 2, but it's position 3 overall)

# Note: Need to check actual variable positions in combined matrix
cat("Bootstrap SE for never-jointly-observed pair:\n")
cat(sprintf("  Estimate: %.3f\n", Sigma_hat[1,3]))
cat(sprintf("  Bootstrap SE: %.3f\n", Sigma_se[1,3]))
cat(sprintf("  95%% CI: [%.3f, %.3f]\n",
    Sigma_hat[1,3] - 1.96*Sigma_se[1,3],
    Sigma_hat[1,3] + 1.96*Sigma_se[1,3]))

# Z-test
z_stat <- Sigma_hat[1,3] / Sigma_se[1,3]
p_value <- 2 * pnorm(abs(z_stat), lower.tail = FALSE)
cat(sprintf("  Z-statistic: %.2f, p-value: %.4f\n", z_stat, p_value))
```

### Bootstrap Diagnostics

```{r bootstrap_diagnostics}
# Check bootstrap metadata
meta <- fit_boot$bootstrap
cat(sprintf("Bootstrap replicates: %d successful, %d failed\n",
    meta$successful, meta$failed))
cat(sprintf("Failure rate: %.1f%%\n",
    100 * meta$failed / (meta$successful + meta$failed)))

# High failure rate (>10%) suggests:
# - Poorly conditioned problem
# - Need more regularization (increase ridge)
# - Design issues (insufficient overlap)
```

---

## Method 3: SEM Standard Errors (Experimental)

### Theory

Supplemented EM (SEM; Meng & Rubin, 1991) computes asymptotic SEs using:

$$
\mathcal{I}_{obs} = \mathcal{I}_{com} - \mathcal{I}_{com} \mathbf{R} (\mathbf{I} - \mathbf{R})^{-1}
$$

where:
- $\mathcal{I}_{obs}$ = observed information matrix
- $\mathcal{I}_{com}$ = complete-data information
- $\mathbf{R}$ = EM rate matrix (estimated via finite differences)

### Configuration

```{r sem_config, eval=FALSE}
fit_sem <- fit_covcomb(
  S_list, nu,
  se_method = "sem",
  control = list(
    sem = list(
      h = 1e-6,      # Finite difference step size
      ridge = 1e-8   # Numerical stability parameter
    )
  )
)
```

### Diagnostics

```{r sem_diagnostics, eval=FALSE}
# Check SEM quality
if (!is.null(fit_sem$sem)) {
  cat("SEM Diagnostics:\n")
  cat(sprintf("  Condition number: %.2e\n", fit_sem$sem$condition_number))
  cat(sprintf("  Min eigenvalue: %.2e\n", fit_sem$sem$min_eigenvalue))

  # High condition number (> 1e6) → unreliable SEs
  # Negative eigenvalue → numerical issues
}
```

### Validation Strategy

**NEVER trust SEM without validation!**

```{r sem_validation, eval=FALSE}
# 1. Fit on subset with SEM (fast)
fit_sem <- fit_covcomb(S_list, nu, se_method = "sem")

# 2. Fit same subset with bootstrap (slower but correct)
fit_boot_subset <- fit_covcomb(S_list, nu, se_method = "bootstrap",
                               control = list(bootstrap = list(B = 100)))

# 3. Compare SEs
se_comparison <- data.frame(
  SEM = as.vector(fit_sem$Sigma_se),
  Bootstrap = as.vector(fit_boot_subset$Sigma_se)
)

# 4. Check correlation and bias
cor_sem_boot <- cor(se_comparison$SEM, se_comparison$Bootstrap, use = "complete.obs")
cat(sprintf("SEM-Bootstrap correlation: %.3f\n", cor_sem_boot))

# Good: correlation > 0.95
# Acceptable: correlation > 0.90
# Poor: correlation < 0.90 → don't use SEM!
```

---

## Method Comparison Study

### Simulation Setup

```{r comparison_setup}
# Realistic scenario: 3 studies, moderate overlap
set.seed(2025)
p <- 8
var_names <- paste0("Var", 1:p)

# True covariance with block structure
true_Sigma <- diag(p)
true_Sigma[1:4, 1:4] <- 0.6
true_Sigma[5:8, 5:8] <- 0.5
diag(true_Sigma) <- 1
dimnames(true_Sigma) <- list(var_names, var_names)

# Three studies with partial overlap
nu_vec <- c(Study1 = 100, Study2 = 120, Study3 = 90)

S1 <- rWishart(1, nu_vec[1], true_Sigma[1:6, 1:6])[,,1] / nu_vec[1]
dimnames(S1) <- list(var_names[1:6], var_names[1:6])

S2 <- rWishart(1, nu_vec[2], true_Sigma[3:8, 3:8])[,,1] / nu_vec[2]
dimnames(S2) <- list(var_names[3:8], var_names[3:8])

S3 <- rWishart(1, nu_vec[3], true_Sigma[c(1,2,5,6,7,8), c(1,2,5,6,7,8)])[,,1] / nu_vec[3]
dimnames(S3) <- list(var_names[c(1,2,5,6,7,8)], var_names[c(1,2,5,6,7,8)])

S_list <- list(Study1 = S1, Study2 = S2, Study3 = S3)
```

### Runtime Comparison

```{r runtime_comparison}
# Measure runtime for each method
cat("Runtime comparison (approximate):\n\n")

# None
time_none <- system.time({
  fit_none <- fit_covcomb(S_list, nu_vec, se_method = "none")
})
cat(sprintf("  none:      %.2f seconds\n", time_none["elapsed"]))

# Plugin
time_plugin <- system.time({
  fit_plugin <- fit_covcomb(S_list, nu_vec, se_method = "plugin")
})
cat(sprintf("  plugin:    %.2f seconds (%.1fx slower than none)\n",
    time_plugin["elapsed"], time_plugin["elapsed"]/time_none["elapsed"]))

# Bootstrap (small B for vignette)
time_boot <- system.time({
  fit_boot <- fit_covcomb(S_list, nu_vec, se_method = "bootstrap",
                          control = list(bootstrap = list(B = 50, progress = FALSE)))
})
cat(sprintf("  bootstrap: %.2f seconds (%.1fx slower than plugin)\n",
    time_boot["elapsed"], time_boot["elapsed"]/time_plugin["elapsed"]))

cat("\n  Note: B=50 used for speed. Use B=200 (4x slower) for production.\n")
```

### Accuracy Comparison

```{r accuracy_comparison, eval=FALSE}
# Compare SEs for a well-observed entry vs poorly-observed entry
cat("\nStandard error comparison:\n")
cat("\nVar1--Var2 (directly observed in Study1):\n")
cat(sprintf("  Plugin:    %.4f\n", fit_plugin$Sigma_se[1,2]))
cat(sprintf("  Bootstrap: %.4f\n", fit_boot$Sigma_se[1,2]))
cat(sprintf("  Ratio:     %.2f (plugin/bootstrap)\n",
    fit_plugin$Sigma_se[1,2] / fit_boot$Sigma_se[1,2]))

cat("\nVar1--Var8 (never jointly observed):\n")
cat(sprintf("  Plugin:    %s\n",
    ifelse(is.na(fit_plugin$Sigma_se[1,8]), "NA", sprintf("%.4f", fit_plugin$Sigma_se[1,8]))))
cat(sprintf("  Bootstrap: %.4f\n", fit_boot$Sigma_se[1,8]))

cat("\n  Plugin returns NA for unobserved pairs (honest uncertainty).\n")
cat("  Bootstrap provides valid SE by accounting for imputation.\n")
```

---

## Practical Recommendations

### For Publication

**Always use `bootstrap`** for:
- Hypothesis tests (p-values)
- Confidence intervals
- Any formal statistical inference
- Entries with sparse observations

```{r publication_example, eval=FALSE}
# Production settings
fit_pub <- fit_covcomb(
  S_list, nu_vec,
  se_method = "bootstrap",
  control = list(
    max_iter = 1000,
    tol = 1e-7,
    bootstrap = list(
      B = 200,        # Standard
      seed = 12345,   # Reproducible
      progress = TRUE # Monitor long runs
    )
  )
)
```

### For Exploration

**Use `plugin`** for:
- Model selection (comparing fits)
- Checking convergence
- Quick diagnostics
- Identifying problematic entries

```{r exploration_example, eval=FALSE}
# Quick diagnostic
fit_diag <- fit_covcomb(S_list, nu_vec, se_method = "plugin")

# Check which entries have large relative uncertainty
rel_se <- fit_diag$Sigma_se / abs(coef(fit_diag))
high_uncertainty <- which(rel_se > 0.5, arr.ind = TRUE)

cat("Entries with relative SE > 50%:\n")
print(high_uncertainty)
```

### For Large Problems

**Use `sem`** (with validation) when:
- Bootstrap runtime > 1 hour
- p > 100
- You need quick turnaround
- You validate on subset first

```{r large_problem, eval=FALSE}
# Large problem workflow
# 1. Quick fit with SEM
fit_sem <- fit_covcomb(large_S_list, large_nu, se_method = "sem")

# 2. Validate on subset
subset_idx <- 1:20  # First 20 variables
fit_boot_subset <- fit_covcomb(
  lapply(large_S_list, function(S) S[subset_idx, subset_idx]),
  large_nu,
  se_method = "bootstrap"
)

# 3. Check diagnostics (condition number < 1e6?)
# 4. Compare SEM vs bootstrap on subset
# 5. Only proceed with SEM if validation passes
```

---

## Common Pitfalls

### Pitfall 1: Using Plugin for Imputed Entries

```{r pitfall1, eval=FALSE}
# BAD: Publishing p-values from plugin SEs
fit <- fit_covcomb(S_list, nu, se_method = "plugin")
z <- fit$Sigma_hat[i,j] / fit$Sigma_se[i,j]  # Anti-conservative!
p_value <- 2*pnorm(abs(z), lower.tail = FALSE)  # Too small!

# GOOD: Use bootstrap
fit <- fit_covcomb(S_list, nu, se_method = "bootstrap")
z <- fit$Sigma_hat[i,j] / fit$Sigma_se[i,j]  # Valid
p_value <- 2*pnorm(abs(z), lower.tail = FALSE)  # Correct Type I error
```

### Pitfall 2: Insufficient Bootstrap Replicates

```{r pitfall2, eval=FALSE}
# BAD: B too small for precision
fit <- fit_covcomb(S_list, nu, se_method = "bootstrap",
                  control = list(bootstrap = list(B = 10)))  # Monte Carlo error!

# GOOD: B = 200 standard
fit <- fit_covcomb(S_list, nu, se_method = "bootstrap",
                  control = list(bootstrap = list(B = 200)))
```

### Pitfall 3: Trusting SEM Without Validation

```{r pitfall3, eval=FALSE}
# BAD: Using SEM blindly
fit <- fit_covcomb(S_list, nu, se_method = "sem")
# Publish results without checking diagnostics

# GOOD: Always validate
fit_sem <- fit_covcomb(S_list, nu, se_method = "sem")
cat("Condition number:", fit_sem$sem$condition_number, "\n")
if (fit_sem$sem$condition_number > 1e6) {
  warning("SEM may be unreliable - validate against bootstrap!")
}
```

---

## Summary

**Quick Reference Table:**

| Scenario | Method | Configuration |
|----------|--------|---------------|
| Publication (small p) | bootstrap | B=200, seed for reproducibility |
| Publication (large p) | bootstrap or sem | B=100-200; validate SEM first |
| Exploration | plugin | Default settings |
| Model selection | plugin | Compare relative magnitudes |
| Large sparse overlap | bootstrap | B=200, expect some NAs |
| Never-observed pairs | bootstrap | Only method providing valid SEs |
| Computational limits | sem | Validate on subset, check diagnostics |

**Golden Rules:**

1. **Never publish with plugin SEs** - anti-conservative for imputed entries
2. **Bootstrap is almost always correct** - worth the wait for formal inference
3. **SEM requires validation** - always check diagnostics and compare to bootstrap
4. **Plugin NAs are honest** - acknowledging true uncertainty about unobserved pairs
5. **More bootstrap replicates ≠ better** - B=200 is standard, B=500 is plenty

---

## References

- Meng, X.-L., & Rubin, D. B. (1991). Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm. *Journal of the American Statistical Association*, 86(416), 899-909.
- McLachlan, G. J., & Krishnan, T. (2008). *The EM Algorithm and Extensions* (2nd ed.). Wiley.
- Efron, B., & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. Chapman & Hall/CRC.

---

## See Also

- `vignette("CovCombR-vignette")` - Package introduction
- `vignette("advanced-configuration")` - Scale parameters and control options
- `vignette("study-design-guide")` - Designing studies for valid inference
- `?fit_covcomb` - Complete function reference
